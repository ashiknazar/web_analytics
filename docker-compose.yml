version: '3.8'

services:
  # -------------------
  # ZOOKEEPER
  # -------------------
  zookeeper:
    image: zookeeper:3.9.3
    container_name: zookeeper
    ports:
      - "2181:2181"
    restart: unless-stopped

  # -------------------
  # KAFKA
  # -------------------
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    restart: unless-stopped

  # -------------------
  # PRODUCER
  # -------------------
  producer:
    build: ./flask-app
    container_name: producer
    depends_on:
      - kafka
    restart: unless-stopped
  kafka-consumer:
    build: ./kafka
    container_name: kafka-consumer
    depends_on: 
      - kafka
      - hdfs-namenode
    restart: unless-stopped


  # -------------------
  # HDFS NAMENODE
  # -------------------
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    environment:
      - CLUSTER_NAME=demo
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_namenode_rpc-address=hdfs-namenode:8020
      - HDFS_CONF_dfs_namenode_http-address=hdfs-namenode:50070
    ports:
      - "50070:50070"
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    restart: unless-stopped

  # -------------------
  # HDFS DATANODE
  # -------------------
  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: hdfs-datanode
    hostname: hdfs-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_namenode_rpc-address=hdfs-namenode:8020
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
    depends_on:
      - hdfs-namenode
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    restart: unless-stopped

  # -------------------
  # STORM
  # -------------------
  storm:
    image: storm:latest
    container_name: storm
    depends_on:
      - zookeeper
    command: ["storm", "nimbus"]
    ports:
      - "6627:6627"
      - "8080:8080"
    restart: unless-stopped

  # -------------------
  # SPARK MASTER
  # -------------------
  spark:
    image: apache/spark:3.5.1
    container_name: spark
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "8081:8080"
    restart: unless-stopped

  # -------------------
  # HIVE
  # -------------------
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    ports:
      - "10000:10000"
    depends_on:
      - hdfs-namenode
      - hdfs-datanode
    restart: unless-stopped

volumes:
  hdfs_namenode:
  hdfs_datanode: